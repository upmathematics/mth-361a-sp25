---
title: "Inference for Difference of Two Means"
subtitle: "Applied Statistics"
author: "MTH-361A | Spring 2025 | University of Portland"
date: "March 26, 2025"
output:
  slidy_presentation:
    font_adjustment: +5
    footer: "| MTH-361A Spring 2025 | <a href='../../index.html'>Back to the Course Website</a>"
    css: ../_style.css
bibliography: ../../references.bib
csl: ../../apa.csl
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r echo=FALSE, eval=TRUE,message=FALSE, warning=FALSE}
library(tidyverse)
library(openintro)
library(gridExtra)
library(latex2exp)
library(kableExtra)
data(COL)
seed <- 42
```

## Objectives

:::: {.column width=15%}
::::

:::: {.column width=70%}
- **Develop an understanding on computing confidence intervals for difference in means**
- **Know how to determine the degrees of freedom for two means**
- **Activity: Determine Confidence Intervals for Difference in Means**
::::

:::: {.column width=15%}
::::

## Previously... (1/2)

**The $t$-distribution**

```{r tDistConvergeToNormalDist, fig.cap="The larger the degrees of freedom the more closely the $t$-distribution resembles the standard normal distribution.", fig.asp=0.5, fig.align='center', message=FALSE, warning=FALSE, out.width="90%"}

plot(c(-5, 5),
  c(0, dnorm(0)),
  type = "n", ylab = "", xlab = "",
  axes = FALSE
)
at <- seq(-10, 10, 2)
axis(1, at)
axis(1, at - 1, rep("", length(at)), tcl = -0.1)
abline(h = 0)

COL. <- fadeColor(IMSCOL["blue", "full"], c("FF", "89", "68", "4C", "33"))
COLt <- fadeColor(IMSCOL["blue", "full"], c("FF", "AA", "85", "60", "45"))
DF <- c("normal", 8, 4, 2, 1)

X <- seq(-10, 10, 0.02)
Y <- dnorm(X)
lines(X, Y, col = COL.[1])

for (i in 2:5) {
  Y <- dt(X, as.numeric(DF[i]))
  lines(X, Y, col = COL.[i], lwd = 1.5)
}

legend(2.5, 0.4,
  legend = c(
    DF[1],
    paste("t, df = ", DF[2:5], sep = "")
  ),
  col = COL.,
  text.col = COLt,
  lty = rep(1, 5),
  lwd = 1.5
)
```

## Previously... (2/2)

**Confidence Intervals for One Mean**

$$
\begin{aligned}
\bar{x} \ &\pm\  t^*_{df} \times \frac{s}{\sqrt{n}}
\end{aligned}
$$

\[
\begin{aligned}
\bar{x} & \longrightarrow \text{sample mean (point estimate)} \\
s & \longrightarrow \text{sample standard deviation} \\
n & \longrightarrow \text{sample size} \\
t^*_{df} & \longrightarrow \text{critical value (t-distribution with degrees of freedom } df \text{)} 
\end{aligned}
\]

## Case Study I: Fuel Efficiency in the City

The problem shown below was taken and slightly modified from your textbook [OpenIntro: Introduction to Modern Statistics Section 20.6](https://openintro-ims.netlify.app/inference-two-means.html#chp20-exercises){target="_blank"}. Consider the research study described below.

> Each year the US Environmental Protection Agency (EPA) releases fuel economy data on cars manufactured in that year. Below are summary statistics on fuel efficiency (in miles/gallon) from random samples of cars with manual and automatic transmissions manufactured in 2021. Do these data provide strong evidence of a difference between the average fuel efficiency of cars with manual and automatic transmissions in terms of their average city mileage? [US DOE EPA 2021](https://www.fueleconomy.gov/feg/download.shtml){target="_blank"}

We will compute the 95\% confidence interval for the true difference in means $\mu_{automatic} - \mu_{manual}$.

```{r fig.align='center', message=FALSE, warning=FALSE}
set.seed(1234)
epa2021_sample <- epa2021 %>%
  filter(transmission_desc %in% c("Manual", "Automatic")) %>%
  group_by(transmission_desc) %>%
  sample_n(size = 25)

epa2021_sample %>%
  group_by(transmission_desc) %>%
  summarise(
    Mean = mean(city_mpg),
    SD   = sd(city_mpg),
    n    = n()
  ) %>%
  kbl(linesep = "", booktabs = TRUE, align = "lccc", col.names = c("CITY", "Mean", "SD", "n"), digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = "HOLD_position",
                full_width = FALSE)
```

## Case Study I: Conditions

  * Conditions.
    - **Independence** (extended). The data are independent within and between the two groups, e.g., the data come from independent random samples or from a randomized experiment.
    - **Normality**. We need large enough sample size for each group. We check the extreme outliers for each group separately.
    
```{r fig.align='center', fig.width=5, fig.height=3, out.width="60%", message=FALSE, warning=FALSE}
ggplot(epa2021_sample, aes(y = fct_rev(transmission_desc), x = city_mpg)) +
  geom_boxplot() +
  labs(y = NULL, x = "City mileage (MPG)")
```

Here, we see two outliers in the manual group. However, both groups shows decent distributions with balanced outliers where - in this case with 25 samples each - we can "ignore" the outliers and assume normality of the sampling distribution of the means.
    
## Case Study I: Two-sample t-interval (1/3)

**Two-sample t-intervals**

$$
\begin{aligned}
\text{point estimate} \ &\pm\  t^*_{df} \times SE \\
\bar{x}_1 - \bar{x}_2 \ &\pm\  t^*_{df} \times \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}
\end{aligned}
$$

  * The **margin of error** is $SE = t^*_{df} \times \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$ where $t^*_{df}$ is calculated from a specified percentile on the t-distribution with *df* degrees of freedom.
  * The official formula for the **degrees of freedom** is quite complex and is generally computed using R, so instead you may use the smaller of $n_1 - 1$ and $n_2 - 1$ for convenience if $n_1 \approx n_2$.
    
## Case Study I: Two-sample t-interval (2/3)

  * Standard Error
    \[
    \begin{aligned}
    SE & = \sqrt{\frac{s_{automatic}^2}{n_{automatic}} + \frac{s_{manual}^2}{n_{manual}}} \\
       & = \sqrt{\frac{3.44^2}{25} + \frac{4.58^2}{25}} \\
    SE & = 1.1456
    \end{aligned}
    \]
    
  * Degrees of freedom is $df = 24$.
    
  * For a 95% confidence level, we find the the critical $t^*_{df}$ where the upper tail is equal to 2.5%: $t^*_{24} = 2.0639.$ The area below $t^*_{24} = -2.0639$ will also be equal to 2.5%.

**Using R to find the critical $t^*$**

```{r echo = TRUE}
cl <- 0.95 # confidence level
lt <- (1-cl)/2 # lower tail probability
df <- 24 # degrees of freedom
qt(lt, df) # t-star
```
  * *A note on the degrees of freedom:* Our example shows two equal sample sizes in each group. So, the degrees of freedom is $25 - 1 = 24$.
  
## Case Study I: Two-sample t-interval (3/3)
  
  * The 95\% Confidence interval is computed as.
  \[
  \begin{aligned}
  \bar{x}_{automatic} - \bar{x}_{manual} & \pm ME \\
                             17.4 - 22.7 & \pm t^*_{df} \times \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}} \\
                             17.4 - 22.7 & \pm 2.0639 \times 1.1456 \\
                                    -5.3 & \pm 2.3644
  \end{aligned}
  \]
  $$(-7.6644,-2.9356)$$
    
Therefore, we are 95\% confident that the true difference in mean fuel efficiency (miles/gallon) between automatic and manual cars is between 2.9356 and 7.664 in absolute value. 

Note that the values are originally negative because how the order of difference terms are computed, meaning a negative difference indicate that there is more efficiency in cars with manual transmission than automatic transmission.

## Degrees of Freedom

**One-sample t-interval**

$$df = n - 1$$

**Two-sample t-interval**

* **Minimum sample size.** If $n_1 \approx n_2$, then $df = \min{\left(n_1 - 1,n_2 - 1\right)}$. *Using this $df$ yields low statistical power.*
* **Pooled.** If $n_1 \ne n_2$ and $s_1 = s_2$, then $df = n_1 + n_2 - 2$.
* **Welch's Formula.** If $n_1 \ne n_2$ and $s_1 \ne s_2$, then $df = \frac{\left( \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2}{\left(\frac{1}{n_1 - 1}\right) \left( \frac{s_1^2}{n_1} \right)^2 + \left(\frac{1}{n_2 - 1}\right) \left( \frac{s_2^2}{n_2} \right)^2}$. *Using this $df$ is the default*.

If the population variance is unknown, use the sample variance $s^2$. If the population variance is known, use population variance $\sigma^2$. Most real-world problems involve sample variances, especially for statistical inference.

## Case Study II

Every year, the US releases to the public a large data set containing information on births recorded in the country. This data set has been of interest to medical researchers who are studying the relation between habits and practices of expectant mothers and the birth of their children. We will work with a random sample of 1,000 cases from the data set released in 2014.

Here are four examples in the data set.

```{r babySmokeDF}
births14 %>% 
  select(-premie, -mature, -lowbirthweight, -whitemom, -marital,-gained) %>%
  head(4) %>%
  kable()
```

## Case Study II: Baby Weights - Smoker vs Non-Smoker

We would like to know, is there convincing evidence that newborns from mothers who smoke have a different average birth weight than newborns from mothers who don't smoke?

Here is the summary statistics for the dataset.

```{r births14-summary-stats}
births14_nona <- births14 %>%
  filter(
    !is.na(habit), !is.na(weight)
  )

births14_sample_stats <- births14_nona %>%
  group_by(habit) %>%
  summarise(
    n = n(),
    Mean = mean(weight),
    SD = sd(weight)
  )

xbar_nonsmoker  <- round(births14_sample_stats %>% filter(habit == "nonsmoker") %>% pull(Mean), 2)
xbar_smoker     <- round(births14_sample_stats %>% filter(habit == "smoker") %>% pull(Mean), 2)
xbar_difference <- xbar_nonsmoker - xbar_smoker
sd_nonsmoker    <- round(births14_sample_stats %>% filter(habit == "nonsmoker") %>% pull(SD), 2)
sd_smoker       <- round(births14_sample_stats %>% filter(habit == "smoker") %>% pull(SD), 2)
n_nonsmoker     <- births14_sample_stats %>% filter(habit == "nonsmoker") %>% pull(n)
n_smoker        <- births14_sample_stats %>% filter(habit == "smoker") %>% pull(n)
se_difference   <- round(sqrt(sd_nonsmoker^2/n_nonsmoker + sd_smoker^2/n_smoker), 2)
t_difference    <- round(xbar_difference / se_difference, 2)
df_difference   <- min(n_nonsmoker-1, n_smoker-1)

births14_sample_stats %>%
  kable()
```

## Case Study II: CLT Conditions

Conditions:

* The data come from a simple random sample, the observations are independent, both within and between samples.
* Both groups over 30 observations, we inspect the data for any particularly extreme outliers and find none.

Since both conditions are satisfied, the difference in sample means may be modeled using a $t$-distribution.

## Case Study II: Examining the Distributions (1/2)

```{r babySmokePlotOfTwoGroupsToExamineSkew, fig.cap="The top panel represents birth weights for infants whose mothers smoked during pregnancy. The bottom panel represents the birth weights for infants whose mothers who did not smoke during pregnancy.", message=FALSE, warning=FALSE, fig.align='center', fig.width=4,fig.height=3, out.width="50%"}
births14_nona %>%
  mutate(habit = fct_rev(if_else(habit == "smoker", "Mothers who smoked", "Mothers who did not smoke"))) %>%
  ggplot(aes(x = weight,fill=habit)) +
  geom_histogram(binwidth = 1) +
  facet_wrap(~habit, ncol = 1, scales = "free_y") +
  scale_x_continuous(breaks = seq(0, 10, 2), limits = c(0, 10)) +
  labs(x = "Newborn weights (lbs)", y = "Count") + 
  scale_fill_discrete(guide="none") + 
  theme_minimal()
```

## Case Study II: Examining the Distributions (2/2)

```{r boxplotWeights, message=FALSE, warning=FALSE, fig.align='center', fig.width=6,fig.height=3, out.width="50%"}
ggplot(births14_nona, aes(x = weight, y = habit,fill=habit)) +
  geom_boxplot() +
  labs(title = "Boxplot of weights broken down by smoking status.", x = "Newborn weights (lbs)", y = "") + 
  scale_fill_discrete(guide="none") + 
  theme_minimal()
```

## Case Study II: Two Sample t-test (1/4)

```{r boxplotWeights2, message=FALSE, warning=FALSE, fig.align='center', fig.width=6,fig.height=3, out.width="50%"}
ggplot(births14_nona, aes(x = weight, y = habit,fill=habit)) +
  geom_boxplot() +
  labs(title = "Boxplot of weights broken down by smoking status.", x = "Newborn weights (lbs)", y = "") + 
  scale_fill_discrete(guide="none") + 
  theme_minimal()
```

```{r}
births14_sample_stats %>%
  kable()
```

Is there a difference in weight means between the smoking group and nonsmoking group?

## Case Study II: Two Sample t-test (2/4)

* Null Hypothesis $H_0$: There is no difference in means between the smoking and nonsmoking groups.
    $$\mu_{smoking} = \mu_{nonsmoking}$$
* Null Hypothesis $H_A$: There is a significant difference in means between the smoking and nonsmoking groups. In particular the smoking group weights is less than the nonsmoking group weights.
    $$\mu_{smoking} < \mu_{nonsmoking}$$
* The null value is $\mu_0 = 0$. The point-estimate is $\bar{x}_{smoking} - \bar{x}_{nonsmoking} = -0.5927$ and the sample standard deviations are $s_{smoking} = 1.5966$ and $s_{nonsmoking} = 1.2328$.
* We set the significance value $\alpha = 0.01$.

## Case Study II: Two Sample t-test (3/4)

  * Compute the standard error
    \[
    \begin{aligned}
    SE & = \sqrt{\frac{s_{smoking}^2}{n_{smoking}} + \frac{s_{nonsmoking}^2}{n_{nonsmoking}}} \\
       & = \sqrt{\frac{1.5966^2}{114} + \frac{1.2328^2}{867}} \\
    SE & = 0.1553
    \end{aligned}
    \]
    
  * Compute the T statistic
    \[
    \begin{aligned}
    t & = \frac{\bar{x}_{smoking} - \bar{x}_{nonsmoking} - \mu_0}{SE} \\
      & = \frac{-0.5927 - 0}{0.1553} \\
    t & = -3.8165
    \end{aligned}
    \]
    
## Case Study II: Two Sample t-test (4/4)

  * Degrees of freedom is $df = min(n_{smoking} - 1,n_{nonsmoking} - 1) = 114 - 1 = 113$.
  * The p-value is $0.0001$.
  
**Using R to compute the p-value**

```{r echo=TRUE}
df <- 113 # degrees of freedom
t <- -3.8165 # test statistic
pt(t,df) # p-value
```

**Conclusions:**
  
  * Since the p-value is less than significance of $0.01$ (the p-value is really small), we can conclude there is is a strong evidence that there is a difference in weights between nonsmoking and smoking groups.
  * Since the T statistic is negative, by the order of how we computed the difference, we can say that the average weights is less in the smoking group than in the nonsmoking group.

## Activity: Determine Confidence Intervals for Difference in Means

1. Make sure you have a copy of the *W 3/26 Worksheet*. This will be handed out physically and it is also digitally available on Moodle.
2. Work on your worksheet by yourself for 10 minutes. Please read the instructions carefully. Ask questions if anything need clarifications.
3. Get together with another student.
4. Discuss your results.
5. Submit your worksheet on Moodle as a `.pdf` file.

## References

::: {#refs}
:::
