---
title: "Inference for Means"
subtitle: "Applied Statistics"
author: "MTH-361A | Spring 2025 | University of Portland"
date: "March 24, 2025"
output:
  slidy_presentation:
    font_adjustment: +5
    footer: "| MTH-361A Spring 2025 | <a href='../../index.html'>Back to the Course Website</a>"
    css: ../_style.css
bibliography: ../../references.bib
csl: ../../apa.csl
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

```{r echo=FALSE, eval=TRUE,message=FALSE, warning=FALSE}
library(tidyverse)
library(openintro)
library(gridExtra)
library(latex2exp)
library(kableExtra)
data(COL)
seed <- 42
```

## Objectives

:::: {.column width=15%}
::::

:::: {.column width=70%}
- **Develop an understanding of t-distributions**
- **Know how to compute confidence intervals for means**
- **Activity: Determine Confidence Intervals for Means**
::::

:::: {.column width=15%}
::::

## Previously... (1/2)

**Inference for Proportions**

\[
\begin{aligned}
\text{key assumptions} & \longrightarrow \text{random samples, large sample size, independence} \\
\text{special assumptions} & \longrightarrow \text{number of "success" and "failure" outcomes is large} \\
\text{population proportion} & \longrightarrow p \\
\text{sample proportion} & \longrightarrow \hat{p} \\
\text{sample size} & \longrightarrow n \\
\text{Confidence Interval} & \longrightarrow \hat{p} \pm z^{\star} \text{SE}_{\hat{p}} \\
\text{null value} & \longrightarrow p_0 \\
\text{null hypothesis} & \longrightarrow H_0: p = p_0 \\
\text{alternative hypothesis} & \longrightarrow H_A: p \ne p_0 \\
\text{standard error} & \longrightarrow \text{SE}_p = \sqrt{\frac{p(1-p)}{n}} \\
\text{test statistic} & \longrightarrow z = \frac{\hat{p} - p_0}{\text{SE}_p}
\end{aligned}
\]

::: {style="color: red;"}
$\star$ "Large" sample size means at least 10.
:::

## Previously... (2/2)

**Inference for Two-Way Tables (Chi-Squared Tests)**

\[
\begin{aligned}
\text{key assumptions} & \longrightarrow \text{random samples, large sample size, independence} \\
\text{special assumptions} & \longrightarrow \text{expected values of each contingency cell is large} \\
\text{null hypothesis} & \longrightarrow \text{two categorical variables are independent} \\
\text{alternative hypothesis} & \longrightarrow \text{two categorical variables are dependent} \\
\text{observed value of a contingency cell} & \longrightarrow O \\
\text{expected value of a contingency cell} & \longrightarrow E \\
\text{test statistic} & \longrightarrow \chi^2 = \sum \frac{(O-E)^2}{E}
\end{aligned}
\]

::: {style="color: red;"}
$\star$ "Large" sample size means at least 10.
:::

## Central Limit Theorem for the Sample Mean

When we collect a sufficiently large sample of $n$ independent observations from a population with mean $\mu$ and standard deviation $\sigma,$ the sampling distribution of $\bar{x}$ will be nearly normal with

$$\text{Mean} = \mu \qquad \text{Standard Error }(SE) = \frac{\sigma}{\sqrt{n}}$$

## Evaluating the two conditions required for modeling $\bar{x}$

Two conditions are required to apply the Central Limit Theorem\index{Central Limit Theorem} for a sample mean $\bar{x}:$

- **Independence.** The sample observations must be independent. The most common way to satisfy this condition is when the sample is a simple random sample from the population.

- **Normality.** When a sample is small, we also require that the sample observations come from a normally distributed population. We can relax this condition more and more for larger and larger sample sizes. *This condition is obviously vague, making it difficult to evaluate, so next we introduce a couple rules of thumb to make checking this condition easier.*

## General rule for performing the normality check

Note, it often takes practice to get a sense for whether or not a normal approximation is appropriate.

-   $\mathbf{n < 30}:$ If the sample size $n$ is less than 30 and there are **no clear outliers** in the data, then we typically assume the data come from a nearly normal distribution to satisfy the condition.
-   $\mathbf{n \geq 30}:$ If the sample size $n$ is at least 30 and there are no **particularly extreme** outliers, then we typically assume the sampling distribution of $\bar{x}$ is nearly normal, even if the underlying distribution of individual observations is not.

## Normality Assesment (1/2)

Consider the four plots provided that come from simple random samples from different populations.

*Are the independence and normality conditions met in each case?*

```{r outliersandsscondition, fig.align='center', fig.cap="Histograms of samples from two different populations.", message=FALSE, warning=FALSE, out.width="50%"}
set.seed(3456)
df1 <- tibble(x = rnorm(15, 3, 2))
df2 <- tibble(x = c(exp(rnorm(49, 0, 0.7)), 22))

p1 <- ggplot(df1, aes(x = x)) +
  geom_histogram(binwidth = 1) +
  labs(x = NULL, y = "Count", title = "Sample 1", subtitle = expression(n[1]~"="~15))

p2 <- ggplot(df2, aes(x = x)) +
  geom_histogram(binwidth = 1) +
  labs(x = NULL, y = "Count", title = "Sample 2", subtitle = expression(n[2]~"="~50))

p3 <- ggplot(df1, aes(x = x)) +
  geom_boxplot() +
  theme(
    axis.text.y = element_blank(), 
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    ) +
  labs(x = NULL)

p4 <- ggplot(df2, aes(x = x)) +
  geom_boxplot() +
  theme(
    axis.text.y = element_blank(), 
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    )+
  labs(x = NULL)

grid.arrange(p1,p2,p3,p4,nrow=2)
```

## Normality Assesment (2/2)

  * The **first sample** has fewer than 30 observations, so we are watching for any clear outliers. *With no clear outliers, the normality condition can be reasonably assumed to be met.*

  * The **second sample** has a sample size greater than 30 and includes an outlier. *This is an example of a particularly extreme outlier, so the normality condition would not be satisfied.*
  
## The t-distribution (1/2)

```{r tDistCompareToNormalDist, fig.asp=0.5, fig.cap="Comparison of a $t$-distribution and a normal distribution.", message=FALSE, warning=FALSE, out.width="90%", fig.align='center'}
df <- tibble(
  x = rep(seq(-5, 5, 0.01), 2),
  distribution = c(rep("Normal distribution", 1001), rep("t-distribution", 1001))
) %>%
  mutate(y = if_else(distribution == "Normal distribution", dnorm(x), dt(x, df = 1)))

ggplot(df, aes(x = x, y = y, color = distribution, linetype = distribution, size = distribution)) +
  geom_hline(yintercept = 0) +
  geom_line() +
  scale_color_manual(values = c(IMSCOL["blue", "full"], IMSCOL["red", "full"])) +
  scale_linetype_manual(values = c("dashed", "solid")) +
  scale_size_manual(values = c(0.5, 1)) +
  theme(
    axis.text = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = c(0.2, 0.7),
    legend.background = element_rect(fill = "white", color = NA),
    legend.text = element_text(size = 12)
  ) +
  labs(x = NULL, y = NULL, color = NULL, linetype = NULL, size = NULL)
```

The $t$-distribution is always centered at zero and has a single parameter: degrees of freedom. The **degrees of freedom** describes the precise form of the bell-shaped $t$-distribution. In general, we'll use a $t$-distribution with $df = n - 1$ to model the sample mean when the sample size is $n.$

## The t-distribution (2/2)

```{r tDistConvergeToNormalDist, fig.cap="The larger the degrees of freedom the more closely the $t$-distribution resembles the standard normal distribution.", fig.asp=0.5, fig.align='center', message=FALSE, warning=FALSE, out.width="90%"}

plot(c(-5, 5),
  c(0, dnorm(0)),
  type = "n", ylab = "", xlab = "",
  axes = FALSE
)
at <- seq(-10, 10, 2)
axis(1, at)
axis(1, at - 1, rep("", length(at)), tcl = -0.1)
abline(h = 0)

COL. <- fadeColor(IMSCOL["blue", "full"], c("FF", "89", "68", "4C", "33"))
COLt <- fadeColor(IMSCOL["blue", "full"], c("FF", "AA", "85", "60", "45"))
DF <- c("normal", 8, 4, 2, 1)

X <- seq(-10, 10, 0.02)
Y <- dnorm(X)
lines(X, Y, col = COL.[1])

for (i in 2:5) {
  Y <- dt(X, as.numeric(DF[i]))
  lines(X, Y, col = COL.[i], lwd = 1.5)
}

legend(2.5, 0.4,
  legend = c(
    DF[1],
    paste("t, df = ", DF[2:5], sep = "")
  ),
  col = COL.,
  text.col = COLt,
  lty = rep(1, 5),
  lwd = 1.5
)
```

## Example 1: Mercury content in Rissoâ€™s dolphins

We will identify a confidence interval for the average mercury content in dolphin muscle using a sample of 19 Risso's dolphins from the Taiji area in Japan.

```{r summaryStatsOfHgInMuscleOfRissosDolphins}
dolphin_summary_stats <- tribble(
  ~n, ~Mean, ~SD, ~Min, ~Max,
  19,    4.4,   2.3,   1.7,   9.2
)

dolphin_summary_stats %>%
  kbl(linesep = "", booktabs = TRUE, 
      caption = "Summary of mercury content in the muscle of 19 Risso's dolphins from the Taiji area. Measurements are in micrograms of mercury per wet gram of muscle $(\\mu$g/wet g).", 
      align = "ccccc") %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), 
               latex_options = c("striped", "hold_position"), full_width = FALSE) %>%
  column_spec(1:5, width = "6em")
```

  >- Question - Are the independence and normality conditions satisfied for this dataset?

  >- The observations are a simple random sample, therefore it is reasonable to assume that the dolphins are independent. The summary statistics do not suggest any clear outliers, with all observations within 2.5 standard deviations of the mean. Based on this evidence, the normality condition seems reasonable.
  
## Example 1: One-sample t-interval (1/2)

**One sample t-intervals**

$$
\begin{aligned}
\text{point estimate} \ &\pm\  t^*_{df} SE \\
\bar{x} \ &\pm\  t^*_{df} \frac{s}{\sqrt{n}}
\end{aligned}
$$

  * We plug in $s$ and $n$ into the formula: $SE = \frac{s}{\sqrt{n}} = \frac{2.3}{\sqrt{19}} = 0.528.$
  
  * The degrees of freedom is easy to calculate: $df = n - 1 = 19 -1 = 18.$ Using statistical software, we find the cutoff where the upper tail is equal to 2.5%: $t^*_{18} = 2.10.$ The area below -2.10 will also be equal to 2.5%.
  
```{r echo = TRUE}
# use qt() to find the t-cutoff (with 95% in the middle)
qt(0.025, df = 18)
qt(0.975, df = 18)
```

## Example 1: One-sample t-interval (2/2)

**One sample t-intervals**

\[
\begin{aligned}
\bar{x} \ &\pm\  t^*_{18} SE \\
4.4 \ &\pm\  2.10 (0.528) \\
\end{aligned} 
\]
$$(3.29,5.51)$$

We are 95% confident the average mercury content of muscles in Risso's dolphins is between 3.29 and 5.51 $\mu$g/wet gram, which is considered extremely high.

## Example 2: Fuel Efficiency in the City

The problem shown below was taken and slightly modified from your textbook [OpenIntro: Introduction to Modern Statistics Section 20.6](https://openintro-ims.netlify.app/inference-two-means.html#chp20-exercises){target="_blank"}. Consider the research study described below.

> Each year the US Environmental Protection Agency (EPA) releases fuel economy data on cars manufactured in that year. Below are summary statistics on fuel efficiency (in miles/gallon) from random samples of cars with manual and automatic transmissions manufactured in 2021. Do these data provide strong evidence of a difference between the average fuel efficiency of cars with manual and automatic transmissions in terms of their average city mileage? [US DOE EPA 2021](https://www.fueleconomy.gov/feg/download.shtml){target="_blank"}

We will compute the 95\% confidence interval for the true difference in means $\mu_{automatic} - \mu_{manual}$.

```{r fig.align='center', message=FALSE, warning=FALSE}
set.seed(1234)
epa2021_sample <- epa2021 %>%
  filter(transmission_desc %in% c("Manual", "Automatic")) %>%
  group_by(transmission_desc) %>%
  sample_n(size = 25)

epa2021_sample %>%
  group_by(transmission_desc) %>%
  summarise(
    Mean = mean(city_mpg),
    SD   = sd(city_mpg),
    n    = n()
  ) %>%
  kbl(linesep = "", booktabs = TRUE, align = "lccc", col.names = c("CITY", "Mean", "SD", "n"), digits = 2) %>%
  kable_styling(bootstrap_options = c("striped", "condensed"), 
                latex_options = "HOLD_position",
                full_width = FALSE)

ggplot(epa2021_sample, aes(y = fct_rev(transmission_desc), x = city_mpg)) +
  geom_boxplot() +
  labs(y = NULL, x = "City mileage (MPG)")
```

## Example 2: Conditions

  * Conditions.
    - **Independence** (extended). The data are independent within and between the two groups, e.g., the data come from independent random samples or from a randomized experiment.
    - **Normality**. We need large enough sample size for each group - at least 30. We check the extreme outliers for each group separately.
    
```{r message=FALSE, warning=FALSE, fig.align='center'}
ggplot(epa2021_sample, aes(y = fct_rev(transmission_desc), x = city_mpg)) +
  geom_boxplot() +
  labs(y = NULL, x = "City mileage (MPG)")
```

Here, we see two outliers in the manual group. However, both groups shows decent distributions with balanced outliers where - in this case - we can "ignore" the outliers and assume normality of the sampling distribution of the means.
    
## Example 2: The Margin of Error (1/3)
    
  * The **standard error** may be computed as
    $$SE = \sqrt{\frac{\sigma_1^2}{n_1} + \frac{\sigma_2^2}{n_2}}$$
    The official formula for the degrees of freedom is quite complex and is generally computed using software, so instead you may use the smaller of $n_1 - 1$ and $n_2 - 1$ for the degrees of freedom if software isn't readily available.
    
  * **Margin of error for** $\bar{x}_1 - \bar{x}_2.$
    The margin of error is $t^*_{df} \sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}$ where $t^*_{df}$ is calculated from a specified percentile on the t-distribution with *df* degrees of freedom.
    
## Example 2: The Margin of Error (2/3)

  * Standard Error
    \[
    \begin{aligned}
    SE & = \sqrt{\frac{s_{automatic}^2}{n_{automatic}} + \frac{s_{manual}^2}{n_{manual}}} \\
       & = \sqrt{\frac{3.44^2}{25} + \frac{4.58^2}{25}} \\
    SE & = 1.1456
    \end{aligned}
    \]
    
  * Margin of Error
    \[
    \begin{aligned}
    ME & = t^*_{24} SE \\
       & = 2.0639 (1.1456) \\
    ME & = 2.3644
    \end{aligned}
    \]
    
## Example 2: The Margin of Error (3/3)
  
  * *A note on the degrees of freedom:* Our example shows two equal sample sizes in each group. So, the degrees of freedom is $25 - 1 = 24$. In some cases where the sample sizes are different, normally we can pick the smaller sample size. This doesnâ€™t bias your CI. It just means the power you have is based on the smaller sample.
  
  * We have talked about power when we learned about decision errors but we will talk more on the power concept later.
  
## Example 2: 95\% Confidence Interval 
  
  * The 95\% Confidence interval is computed as.
  \[
  \begin{aligned}
  \bar{x}_{automatic} - \bar{x}_{manual} \pm ME & = 17.4 - 22.7 \pm 2.3644 \\
                                                & = -5.3 \pm 2.3644
  \end{aligned}
  \]
  $$(-7.6644,-2.9356)$$
    
Therefore, we are 95\% confident that the true difference in mean fuel efficiency (miles/gallon) between automatic and manual cars is between 2.9356 and 7.664 in absolute value. 

Note that the values are originally negative because how the order of difference terms are computed, meaning a negative difference indicate that there is more efficiency in cars with manual transmission than automatic transmission.

## Activity: Determine Confidence Intervals for Means

1. Make sure you have a copy of the *M 3/24 Worksheet*. This will be handed out physically and it is also digitally available on Moodle.
2. Work on your worksheet by yourself for 10 minutes. Please read the instructions carefully. Ask questions if anything need clarifications.
3. Get together with another student.
4. Discuss your results.
5. Submit your worksheet on Moodle as a `.pdf` file.

## References

::: {#refs}
:::
